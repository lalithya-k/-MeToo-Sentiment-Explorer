{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.initializers import Constant\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from ipynb.fs.full.datapreprocessing import preProcessDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cleaned datasets\n",
    "train, test = pd.read_csv('dataset/train.csv'), pd.read_csv('dataset/test.csv')\n",
    "train, test = preProcessDataset(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and pad the sentences from processedText matrix\n",
    "max_features = 31000 #number of words we care about\n",
    "sequence_length = 256 #number of words to be taken from each sentence\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(train['processedText'].values)\n",
    "\n",
    "#create train matrix\n",
    "xTrain = tokenizer.texts_to_sequences(train['processedText'].values)\n",
    "xTrain = pad_sequences(xTrain, sequence_length)\n",
    "\n",
    "#create text matrix\n",
    "xTest = tokenizer.texts_to_sequences(test['processedText'].values)\n",
    "xTest = pad_sequences(xTest, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use GloVe embedding data\n",
    "embeddings_index = {}\n",
    "f = open('PATH TO GloVe Text File')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map tokens to their respective words\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Embedding matrix from the tokenized sentences\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterSizes = [7,8,9]\n",
    "numberOfFilters = 512\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CNN Model\n",
    "inputs = Input(shape=(sequence_length,),dtype='int32')\n",
    "embedding = Embedding(num_words,embedding_dim,embeddings_initializer=Constant(embedding_matrix),input_length=sequence_length,trainable=True)(inputs)\n",
    "reshape = keras.layers.Reshape((sequence_length, embedding_dim,1))(embedding)\n",
    "\n",
    "conv0 = keras.layers.Conv2D(numberOfFilters, kernel_size=(filterSizes[0], embeddingDimension), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv1 = keras.layers.Conv2D(numberOfFilters, kernel_size=(filterSizes[1], embeddingDimension), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv2 = keras.layers.Conv2D(numberOfFilters, kernel_size=(filterSizes[2], embeddingDimension), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxPool0 = keras.layers.MaxPool2D(pool_size=(sequenceLength-filterSizes[0]+1,1), strides=(1,1), padding='valid')(conv0)\n",
    "maxPool1 = keras.layers.MaxPool2D(pool_size=(sequenceLength-filterSizes[1]+1,1), strides=(1,1), padding='valid')(conv1)\n",
    "maxPool2 = keras.layers.MaxPool2D(pool_size=(sequenceLength-filterSizes[2]+1,1), strides=(1,1), padding='valid')(conv2)\n",
    "\n",
    "concatenatedTensor = keras.layers.Concatenate(axis=1)([maxPool0, maxPool1, maxPool2])\n",
    "flatten = keras.layers.Flatten()(concatenatedTensor)\n",
    "dropout = keras.layers.Dropout(dropout)(flatten)\n",
    "output = keras.layers.Dense(units=1, activation='sigmoid')(dropout)\n",
    "\n",
    "classifier = keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model and create predictions and store them in a dictionary\n",
    "preds = {}\n",
    "classes = ['Text_Only_Informative','Image_Only_Informative','Directed_Hate','Generalized_Hate','Sarcasm','Allegation','Justification','Refutation','Support','Oppose']\n",
    "for className in classes:\n",
    "    print('Training and Prediction for the class: ', className)\n",
    "    classifier.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    classifier.fit(xTrain, train[className], batch_size=50, epochs=7, validation_split=0.2)\n",
    "    predictions = classifier.predict(xTest)\n",
    "    pred[className] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create submission dataframe and output csv file\n",
    "a1 = np.array(preds['Text_Only_Informative'])\n",
    "submission = pd.DataFrame(data=a1, index=test['TweeId'], columns=['Text_Only_Informative'])\n",
    "submission['Image_Only_Informative'] = preds['Image_Only_Informative']\n",
    "submission['Directed_Hate'] = preds['Directed_Hate']\n",
    "submission['Generalized_Hate'] = preds['Generalized_Hate']\n",
    "submission['Sarcasm'] = preds['Sarcasm']\n",
    "submission['Allegation'] = preds['Allegation']\n",
    "submission['Justification'] = preds['Justification']\n",
    "submission['Refutation'] = preds['Refutation']\n",
    "submission['Support'] = preds['Support']\n",
    "submission['Oppose'] = preds['Oppose']\n",
    "submission.to_csv('dataset/submissionCNN.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
